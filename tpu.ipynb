{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFtQjv4SzHRj"
      },
      "source": [
        "# TPUs in Colab\n",
        "This tutorial discusses parallelism via `jax.Array`, the unified array object model available in JAX v0.4.1 and newer.\n",
        "\n",
        "Adapted from [this notebook](https://colab.research.google.com/github/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNxScTfq3vGF"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mEKF3zIF3vGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1723114-11b5-41ae-f95f-570333fec4c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcBbkTxaUi7m",
        "outputId": "ed9d006c-6ec5-47d0-ac73-b4d3b70db9ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¦ Step 1: Load and preprocess datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "print(\"ğŸ” TensorFlow version:\", tf.__version__)\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(\"ğŸš€ Running on TPU!\")\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(\"ğŸ§  Running on GPU!\")\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:\n",
        "        print(\"ğŸ’» Running on CPU!\")\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "\n",
        "raw_df = pd.read_csv('/content/drive/MyDrive/natural_disasters_2024.csv')\n",
        "geo_df = pd.read_csv('/content/drive/MyDrive/cleaned_disasters.csv')\n",
        "print(\"âœ… Datasets loaded\")\n",
        "\n",
        "# Strip and convert columns\n",
        "raw_df.columns = raw_df.columns.str.strip()\n",
        "geo_df.columns = geo_df.columns.str.strip()\n",
        "for col in ['Latitude', 'Longitude', 'Magnitude', 'Severity']:\n",
        "    if col in geo_df.columns:\n",
        "        geo_df[col] = geo_df[col].astype(np.float32)\n",
        "    if col in raw_df.columns:\n",
        "        raw_df[col] = raw_df[col].astype(np.float32)\n",
        "\n",
        "print(\"âš™ï¸ Data ready\")\n",
        "\n",
        "# ğŸ§  Step 2: Train TensorFlow DNN on TPU\n",
        "X_train = geo_df[['Latitude', 'Longitude', 'Magnitude']].values\n",
        "y_train = geo_df['Severity'].values\n",
        "\n",
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1)\n",
        "\n",
        "model.save('trained_dnn_model.keras')\n",
        "print(\"âœ… Model trained and saved\")\n",
        "\n",
        "# â›“ Step 3: Run simulation and inference on CPU with NumPy\n",
        "model = tf.keras.models.load_model('trained_dnn_model.keras')\n",
        "\n",
        "stream_batch = raw_df.sample(n=10000).reset_index(drop=True)\n",
        "np.random.seed(42)\n",
        "stream_batch['Latitude'] = np.random.uniform(-90, 90, size=len(stream_batch))\n",
        "stream_batch['Longitude'] = np.random.uniform(-180, 180, size=len(stream_batch))\n",
        "stream_batch['Magnitude'] = stream_batch['Magnitude'].astype(np.float32)\n",
        "\n",
        "X_stream = stream_batch[['Latitude', 'Longitude', 'Magnitude']].values\n",
        "records = stream_batch.to_dict(orient='records')\n",
        "\n",
        "success_count, failure_count = 0, 0\n",
        "send_times, total_bytes_sent = [], 0\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, record in enumerate(records):\n",
        "    try:\n",
        "        features = np.array([[record['Latitude'], record['Longitude'], record['Magnitude']]])\n",
        "        pred = model.predict(features, verbose=0)[0][0]\n",
        "        record['Predicted_Severity'] = float(pred)\n",
        "\n",
        "        t0 = time.time()\n",
        "        size = len(str(record).encode('utf-8'))\n",
        "        time.sleep(0.01)\n",
        "        t1 = time.time()\n",
        "\n",
        "        send_times.append(t1 - t0)\n",
        "        total_bytes_sent += size\n",
        "        success_count += 1\n",
        "    except:\n",
        "        failure_count += 1\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# ğŸ“Š Metrics\n",
        "total_time = end_time - start_time\n",
        "\n",
        "avg_speed = success_count / total_time\n",
        "throughput = total_bytes_sent / total_time\n",
        "\n",
        "min_send = min(send_times)\n",
        "max_send = max(send_times)\n",
        "mean_send = np.mean(send_times)\n",
        "median_send = np.median(send_times)\n",
        "std_send = np.std(send_times)\n",
        "\n",
        "print(f\"ğŸ”¢ Total Records: {len(records)}\")\n",
        "print(f\"âœ… Successfully Sent: {success_count}\")\n",
        "print(f\"âŒ Failed Sends: {failure_count}\")\n",
        "print(f\"â±ï¸ Total Time Taken: {total_time:.2f} seconds\")\n",
        "print(f\"âš¡ Average Send Speed: {avg_speed:.2f} records/second\")\n",
        "print(f\"ğŸ“¦ Total Data Sent: {total_bytes_sent / 1024:.2f} KB\")\n",
        "print(f\"ğŸš€ Throughput: {throughput:.2f} bytes/second\")\n",
        "print(f\"ğŸ“ˆ Min Send Time: {min_send:.4f} sec\")\n",
        "print(f\"ğŸ“‰ Max Send Time: {max_send:.4f} sec\")\n",
        "print(f\"ğŸ“Š Mean Send Time: {mean_send:.4f} sec\")\n",
        "print(f\"ğŸ“ Median Send Time: {median_send:.4f} sec\")\n",
        "print(f\"ğŸ“ Std Dev Send Time: {std_send:.4f} sec\")\n",
        "\n",
        "\n",
        "# ğŸš¨ Save critical alerts\n",
        "final_df = pd.DataFrame(records)\n",
        "alerts = final_df[final_df['Predicted_Severity'] > 8]\n",
        "\n",
        "if len(alerts) > 0:\n",
        "    alerts_path = \"/content/critical_alerts.csv\"\n",
        "    alerts.to_csv(alerts_path, index=False)\n",
        "    from google.colab import files\n",
        "    files.download(alerts_path)\n",
        "    print(f\"âš ï¸ Alerts saved to {alerts_path}\")\n",
        "else:\n",
        "    print(\"âœ… No critical alerts found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eZJ0z_tud3IX",
        "outputId": "c3df7c6d-628a-4808-d500-e6fdce1275b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” TensorFlow version: 2.19.0\n",
            "ğŸ’» Running on CPU!\n",
            "âœ… Datasets loaded\n",
            "âš™ï¸ Data ready\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 104.7915\n",
            "Epoch 2/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2518\n",
            "Epoch 3/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2708\n",
            "Epoch 4/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2475\n",
            "Epoch 5/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0716\n",
            "Epoch 6/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7987\n",
            "Epoch 7/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7037\n",
            "Epoch 8/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6620\n",
            "Epoch 9/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6543\n",
            "Epoch 10/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6342\n",
            "Epoch 11/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6474\n",
            "Epoch 12/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6120\n",
            "Epoch 13/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5885\n",
            "Epoch 14/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5841\n",
            "Epoch 15/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5682\n",
            "Epoch 16/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5850\n",
            "Epoch 17/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5846\n",
            "Epoch 18/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5762\n",
            "Epoch 19/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5759\n",
            "Epoch 20/20\n",
            "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5679\n",
            "âœ… Model trained and saved\n",
            "ğŸ”¢ Total Records: 10000\n",
            "âœ… Successfully Sent: 10000\n",
            "âŒ Failed Sends: 0\n",
            "â±ï¸ Total Time Taken: 1021.05 seconds\n",
            "âš¡ Average Send Speed: 9.79 records/second\n",
            "ğŸ“¦ Total Data Sent: 2921.03 KB\n",
            "ğŸš€ Throughput: 2929.48 bytes/second\n",
            "ğŸ“ˆ Min Send Time: 0.0100 sec\n",
            "ğŸ“‰ Max Send Time: 0.0111 sec\n",
            "ğŸ“Š Mean Send Time: 0.0101 sec\n",
            "ğŸ“ Median Send Time: 0.0101 sec\n",
            "ğŸ“ Std Dev Send Time: 0.0000 sec\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f4792163-2a70-4a62-ad1e-30ac8b05e955\", \"critical_alerts.csv\", 304506)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Alerts saved to /content/critical_alerts.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "name": "TPUs in Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}