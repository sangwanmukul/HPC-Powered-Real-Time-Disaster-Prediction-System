{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "67T0090Jk2KL"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0C8IV5TQnjN",
        "outputId": "142e856c-e05f-477f-dfd2-51361621c544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 592, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 592 (delta 125), reused 82 (delta 82), pack-reused 434 (from 3)\u001b[K\n",
            "Receiving objects: 100% (592/592), 194.79 KiB | 1.62 MiB/s, done.\n",
            "Resolving deltas: 100% (299/299), done.\n",
            "Installing RAPIDS remaining 25.04 libraries\n",
            "Using Python 3.11.12 environment at: /usr\n",
            "Resolved 173 packages in 14.53s\n",
            "Downloading rmm-cu12 (1.5MiB)\n",
            "Downloading cudf-cu12 (1.7MiB)\n",
            "Downloading ucx-py-cu12 (2.2MiB)\n",
            "Downloading libcuspatial-cu12 (31.1MiB)\n",
            "Downloading libcuvs-cu12 (1.1GiB)\n",
            "Downloading pylibcudf-cu12 (26.4MiB)\n",
            "Downloading librmm-cu12 (2.9MiB)\n",
            "Downloading libcudf-cu12 (538.8MiB)\n",
            "Downloading cucim-cu12 (5.6MiB)\n",
            "Downloading libcugraph-cu12 (1.4GiB)\n",
            "Downloading cuspatial-cu12 (4.1MiB)\n",
            "Downloading raft-dask-cu12 (274.9MiB)\n",
            "Downloading cuml-cu12 (9.4MiB)\n",
            "Downloading pylibcugraph-cu12 (2.0MiB)\n",
            "Downloading cuproj-cu12 (1.1MiB)\n",
            "Downloading libkvikio-cu12 (2.0MiB)\n",
            "Downloading libcuml-cu12 (404.9MiB)\n",
            "Downloading libraft-cu12 (20.8MiB)\n",
            "Downloading cugraph-cu12 (3.0MiB)\n",
            "Downloading shapely (2.4MiB)\n",
            "Downloading datashader (17.5MiB)\n",
            "Downloading dask (1.3MiB)\n",
            " Downloaded cuproj-cu12\n",
            " Downloaded shapely\n",
            " Downloaded libkvikio-cu12\n",
            " Downloaded datashader\n",
            " Downloaded ucx-py-cu12\n",
            " Downloaded cudf-cu12\n",
            " Downloaded pylibcugraph-cu12\n",
            " Downloaded dask\n",
            " Downloaded librmm-cu12\n",
            " Downloaded cucim-cu12\n",
            " Downloaded cuspatial-cu12\n",
            " Downloaded rmm-cu12\n",
            " Downloaded cuml-cu12\n",
            " Downloaded cugraph-cu12\n",
            " Downloaded libcuspatial-cu12\n",
            " Downloaded pylibcudf-cu12\n",
            " Downloaded libraft-cu12\n",
            " Downloaded libcudf-cu12\n",
            " Downloaded raft-dask-cu12\n",
            " Downloaded libcuml-cu12\n",
            " Downloaded libcuvs-cu12\n",
            " Downloaded libcugraph-cu12\n",
            "Prepared 51 packages in 1m 18s\n",
            "Uninstalled 28 packages in 689ms\n",
            "Installed 51 packages in 482ms\n",
            " + arrow==1.3.0\n",
            " + cucim-cu12==25.4.0\n",
            " - cudf-cu12==25.2.1 (from https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl)\n",
            " + cudf-cu12==25.4.0\n",
            " + cugraph-cu12==25.4.0\n",
            " - cuml-cu12==25.2.1\n",
            " + cuml-cu12==25.4.0\n",
            " + cuproj-cu12==25.4.0\n",
            " + cuspatial-cu12==25.4.0\n",
            " - cuvs-cu12==25.2.1\n",
            " + cuvs-cu12==25.4.0\n",
            " + cuxfilter-cu12==25.4.0\n",
            " - dask==2024.12.1\n",
            " + dask==2025.2.0\n",
            " - dask-cuda==25.2.0\n",
            " + dask-cuda==25.4.0\n",
            " - dask-cudf-cu12==25.2.2\n",
            " + dask-cudf-cu12==25.4.0\n",
            " + datashader==0.18.0\n",
            " - distributed==2024.12.1\n",
            " + distributed==2025.2.0\n",
            " - distributed-ucxx-cu12==0.42.0\n",
            " + distributed-ucxx-cu12==0.43.0\n",
            " + fqdn==1.5.1\n",
            " + isoduration==20.11.0\n",
            " - jupyter-client==6.1.12\n",
            " + jupyter-client==8.6.3\n",
            " + jupyter-events==0.12.0\n",
            " - jupyter-server==1.16.0\n",
            " + jupyter-server==2.15.0\n",
            " + jupyter-server-proxy==4.4.0\n",
            " + jupyter-server-terminals==0.5.3\n",
            " - libcudf-cu12==25.2.1 (from https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl)\n",
            " + libcudf-cu12==25.4.0\n",
            " - libcugraph-cu12==25.2.0\n",
            " + libcugraph-cu12==25.4.0\n",
            " - libcuml-cu12==25.2.1\n",
            " + libcuml-cu12==25.4.0\n",
            " + libcuspatial-cu12==25.4.0\n",
            " - libcuvs-cu12==25.2.1\n",
            " + libcuvs-cu12==25.4.0\n",
            " - libkvikio-cu12==25.2.1\n",
            " + libkvikio-cu12==25.4.0\n",
            " - libraft-cu12==25.2.0\n",
            " + libraft-cu12==25.4.0\n",
            " + librmm-cu12==25.4.0\n",
            " - libucxx-cu12==0.42.0\n",
            " + libucxx-cu12==0.43.0\n",
            " - numba-cuda==0.2.0\n",
            " + numba-cuda==0.4.0\n",
            " - nx-cugraph-cu12==25.2.0 (from https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl)\n",
            " + nx-cugraph-cu12==25.4.0\n",
            " + overrides==7.7.0\n",
            " + pyct==0.5.0\n",
            " - pylibcudf-cu12==25.2.1 (from https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl)\n",
            " + pylibcudf-cu12==25.4.0\n",
            " - pylibcugraph-cu12==25.2.0\n",
            " + pylibcugraph-cu12==25.4.0\n",
            " - pylibraft-cu12==25.2.0\n",
            " + pylibraft-cu12==25.4.0\n",
            " + python-json-logger==3.3.0\n",
            " - raft-dask-cu12==25.2.0\n",
            " + raft-dask-cu12==25.4.0\n",
            " - rapids-dask-dependency==25.2.0\n",
            " + rapids-dask-dependency==25.4.0\n",
            " + rapids-logger==0.1.1\n",
            " + rfc3339-validator==0.1.4\n",
            " + rfc3986-validator==0.1.1\n",
            " - rmm-cu12==25.2.0\n",
            " + rmm-cu12==25.4.0\n",
            " - shapely==2.1.0\n",
            " + shapely==2.0.7\n",
            " + simpervisor==1.0.0\n",
            " + types-python-dateutil==2.9.0.20241206\n",
            " - ucx-py-cu12==0.42.0\n",
            " + ucx-py-cu12==0.43.0\n",
            " - ucxx-cu12==0.42.0\n",
            " + ucxx-cu12==0.43.0\n",
            " + uri-template==1.3.0\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "\n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "\n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eHd0ovXslS1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4nLrk46BllED",
        "outputId": "f1b73a91-b7c6-4490-cfb3-da2d3a1d1182"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'25.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import cudf\n",
        "cudf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xgAFgI15ddf6",
        "outputId": "f6da37bd-1687-4f9b-a633-0e0da1c3e79b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'25.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import cuml\n",
        "cuml.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GE3Jvj8d3_M",
        "outputId": "6e123d4a-298d-488d-dc41-b676da37f386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tuv4kuo2CwSc",
        "outputId": "1ba2b900-26b6-486a-b31a-d1d913b44834"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔎 TensorFlow version: 2.18.0\n",
            "🧠 Running on GPU!\n",
            "✅ Datasets loaded from Google Drive\n",
            "⚙️ Pandas DataFrames ready\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 20.5093\n",
            "Epoch 2/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7390\n",
            "Epoch 3/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6252\n",
            "Epoch 4/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.6535\n",
            "Epoch 5/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6365\n",
            "Epoch 6/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6807\n",
            "Epoch 7/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6105\n",
            "Epoch 8/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.6429\n",
            "Epoch 9/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6558\n",
            "Epoch 10/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6326\n",
            "Epoch 11/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6338\n",
            "Epoch 12/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6378\n",
            "Epoch 13/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6145\n",
            "Epoch 14/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6671\n",
            "Epoch 15/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6473\n",
            "Epoch 16/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.6149\n",
            "Epoch 17/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.6329\n",
            "Epoch 18/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6022\n",
            "Epoch 19/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6055\n",
            "Epoch 20/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6004\n",
            "✅ TensorFlow DNN model trained and saved\n",
            "🔢 Total Records: 10000\n",
            "✅ Successfully Sent: 10000\n",
            "❌ Failed Sends: 0\n",
            "⏱️ Total Time Taken: 1916.56 seconds\n",
            "⚡ Average Send Speed: 5.22 records/second\n",
            "📦 Total Data Sent: 2921.05 KB\n",
            "🚀 Throughput: 1560.69 bytes/second\n",
            "📈 Min Send Time: 0.0100 sec\n",
            "📉 Max Send Time: 0.0130 sec\n",
            "📊 Mean Send Time: 0.0101 sec\n",
            "📏 Median Send Time: 0.0101 sec\n",
            "📐 Std Dev Send Time: 0.0001 sec\n",
            "⚠️ Critical alerts saved to /content/critical_alerts.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7705a4c8-263d-4726-8980-88cfd623da72\", \"critical_alerts.csv\", 305773)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 📦 Step 1: Load datasets from Google Drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"🔎 TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Device Check\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(\"🚀 Running on TPU!\")\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(\"🧠 Running on GPU!\")\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:\n",
        "        print(\"💻 Running on CPU!\")\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "\n",
        "raw_df = pd.read_csv('/content/drive/MyDrive/natural_disasters_2024.csv')\n",
        "geo_df = pd.read_csv('/content/drive/MyDrive/cleaned_disasters.csv')\n",
        "print(\"✅ Datasets loaded from Google Drive\")\n",
        "\n",
        "# 🛠️ Step 2: Preprocess\n",
        "raw_df.columns = raw_df.columns.str.strip()\n",
        "geo_df.columns = geo_df.columns.str.strip()\n",
        "\n",
        "# Ensure float types\n",
        "for col in ['Latitude', 'Longitude', 'Magnitude', 'Severity']:\n",
        "    if col in geo_df.columns:\n",
        "        geo_df[col] = geo_df[col].astype(np.float32)\n",
        "    if col in raw_df.columns:\n",
        "        raw_df[col] = raw_df[col].astype(np.float32)\n",
        "\n",
        "print(\"⚙️ Pandas DataFrames ready\")\n",
        "\n",
        "# 🧠 Step 3: Train TensorFlow DNN\n",
        "X_train = geo_df[['Latitude', 'Longitude', 'Magnitude']].values\n",
        "y_train = geo_df['Severity'].values\n",
        "\n",
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)  # Regression output\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "model.save('trained_dnn_model.keras')\n",
        "print(\"✅ TensorFlow DNN model trained and saved\")\n",
        "\n",
        "# 📡 Step 4: Simulate streaming data\n",
        "import cupy as cp\n",
        "\n",
        "stream_batch = raw_df.sample(n=10000).reset_index(drop=True)\n",
        "\n",
        "# Randomly generate Latitude, Longitude, Magnitude (Fixed)\n",
        "cp.random.seed(42)\n",
        "stream_batch['Latitude'] = cp.random.uniform(-90, 90, size=len(stream_batch)).get()\n",
        "stream_batch['Longitude'] = cp.random.uniform(-180, 180, size=len(stream_batch)).get()\n",
        "stream_batch['Magnitude'] = cp.asarray(stream_batch['Magnitude']).astype(cp.float32).get()\n",
        "\n",
        "X_stream = stream_batch[['Latitude', 'Longitude', 'Magnitude']].values\n",
        "\n",
        "records = stream_batch.to_dict(orient='records')\n",
        "success_count = 0\n",
        "failure_count = 0\n",
        "send_times = []\n",
        "total_bytes_sent = 0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, record in enumerate(records):\n",
        "    try:\n",
        "        # Predict Severity\n",
        "        features = np.array([[record['Latitude'], record['Longitude'], record['Magnitude']]])\n",
        "        pred = model.predict(features, verbose=0)[0][0]\n",
        "        record['Predicted_Severity'] = float(pred)\n",
        "\n",
        "        # Simulate sending\n",
        "        t0 = time.time()\n",
        "        size = len(str(record).encode('utf-8'))\n",
        "        time.sleep(0.01)  # Artificial latency\n",
        "        t1 = time.time()\n",
        "\n",
        "        send_times.append(t1 - t0)\n",
        "        total_bytes_sent += size\n",
        "        success_count += 1\n",
        "    except Exception as e:\n",
        "        failure_count += 1\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# ⏱️ Step 5: Calculate Metrics\n",
        "total_time = end_time - start_time\n",
        "avg_speed = success_count / total_time if total_time > 0 else 0\n",
        "throughput = total_bytes_sent / total_time if total_time > 0 else 0\n",
        "min_send = min(send_times) if send_times else 0\n",
        "max_send = max(send_times) if send_times else 0\n",
        "mean_send = np.mean(send_times) if send_times else 0\n",
        "median_send = np.median(send_times) if send_times else 0\n",
        "std_send = np.std(send_times) if send_times else 0\n",
        "\n",
        "# 📢 Step 6: Print Metrics\n",
        "print(f\"🔢 Total Records: {len(records)}\")\n",
        "print(f\"✅ Successfully Sent: {success_count}\")\n",
        "print(f\"❌ Failed Sends: {failure_count}\")\n",
        "print(f\"⏱️ Total Time Taken: {total_time:.2f} seconds\")\n",
        "print(f\"⚡ Average Send Speed: {avg_speed:.2f} records/second\")\n",
        "print(f\"📦 Total Data Sent: {total_bytes_sent / 1024:.2f} KB\")\n",
        "print(f\"🚀 Throughput: {throughput:.2f} bytes/second\")\n",
        "print(f\"📈 Min Send Time: {min_send:.4f} sec\")\n",
        "print(f\"📉 Max Send Time: {max_send:.4f} sec\")\n",
        "print(f\"📊 Mean Send Time: {mean_send:.4f} sec\")\n",
        "print(f\"📏 Median Send Time: {median_send:.4f} sec\")\n",
        "print(f\"📐 Std Dev Send Time: {std_send:.4f} sec\")\n",
        "\n",
        "# 🚨 Step 7: Save critical alerts\n",
        "final_df = pd.DataFrame(records)\n",
        "alerts = final_df[final_df['Predicted_Severity'] > 8]\n",
        "\n",
        "if len(alerts) > 0:\n",
        "    alerts_path = \"/content/critical_alerts.csv\"\n",
        "    alerts.to_csv(alerts_path, index=False)\n",
        "    print(f\"⚠️ Critical alerts saved to {alerts_path}\")\n",
        "    from google.colab import files\n",
        "    files.download(alerts_path)\n",
        "else:\n",
        "    print(\"✅ No critical alerts found.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}